{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "from common.common import create_folder\n",
    "from common.pytorch import load_model\n",
    "import pytorch_pretrained_bert as Bert\n",
    "from model.utils import age_vocab\n",
    "from common.common import load_obj\n",
    "from dataLoader.MLM_med import MLMLoader\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from model.MLM_med import BertForMaskedLM\n",
    "from model.optimiser import adam\n",
    "import sklearn.metrics as skm\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings = config.get('max_position_embedding'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        self.age_vocab_size = config.get('age_vocab_size')\n",
    "        self.med_vocab_size = config.get('med_vocab_size')  # OWN EMBEDDINGS\n",
    "        \n",
    "class TrainConfig(object):\n",
    "    def __init__(self, config):\n",
    "        self.batch_size = config.get('batch_size')\n",
    "        self.use_cuda = config.get('use_cuda')\n",
    "        self.max_len_seq = config.get('max_len_seq')\n",
    "        self.train_loader_workers = config.get('train_loader_workers')\n",
    "        self.test_loader_workers = config.get('test_loader_workers')\n",
    "        self.device = config.get('device')\n",
    "        self.output_dir = config.get('output_dir')\n",
    "        self.output_name = config.get('output_name')\n",
    "        self.best_name = config.get('best_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = {\n",
    "    'vocab':'token2idx-added',  # vocabulary idx2token, token2idx\n",
    "    'med_vocab' : 'med-token2idx', \n",
    "    'data': './behrt_format_mimic4ed/',  # formated data \n",
    "    'model_path': 'josa_model', # where to save model\n",
    "    'model_name': 'test_model', # model name\n",
    "    'file_name': 'test_model',  # log path\n",
    "}\n",
    "create_folder(file_config['model_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_params = {\n",
    "    'max_seq_len': 64,\n",
    "    'max_age': 110,\n",
    "    'month': 1, # try to transform data into month-based\n",
    "    'age_symbol': None,\n",
    "    'min_visit': 5, # try to reduce to 3\n",
    "    'gradient_accumulation_steps': 1\n",
    "}\n",
    "\n",
    "optim_param = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'batch_size': 256,\n",
    "    'use_cuda': False,\n",
    "    'max_len_seq': global_params['max_seq_len'],\n",
    "    'device': 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PAD': 0, 'UNK': 1, '0': 2, '1': 3, '2': 4, '3': 5, '4': 6, '5': 7, '6': 8, '7': 9, '8': 10, '9': 11, '10': 12, '11': 13, '12': 14, '13': 15, '14': 16, '15': 17, '16': 18, '17': 19, '18': 20, '19': 21, '20': 22, '21': 23, '22': 24, '23': 25, '24': 26, '25': 27, '26': 28, '27': 29, '28': 30, '29': 31, '30': 32, '31': 33, '32': 34, '33': 35, '34': 36, '35': 37, '36': 38, '37': 39, '38': 40, '39': 41, '40': 42, '41': 43, '42': 44, '43': 45, '44': 46, '45': 47, '46': 48, '47': 49, '48': 50, '49': 51, '50': 52, '51': 53, '52': 54, '53': 55, '54': 56, '55': 57, '56': 58, '57': 59, '58': 60, '59': 61, '60': 62, '61': 63, '62': 64, '63': 65, '64': 66, '65': 67, '66': 68, '67': 69, '68': 70, '69': 71, '70': 72, '71': 73, '72': 74, '73': 75, '74': 76, '75': 77, '76': 78, '77': 79, '78': 80, '79': 81, '80': 82, '81': 83, '82': 84, '83': 85, '84': 86, '85': 87, '86': 88, '87': 89, '88': 90, '89': 91, '90': 92, '91': 93, '92': 94, '93': 95, '94': 96, '95': 97, '96': 98, '97': 99, '98': 100, '99': 101, '100': 102, '101': 103, '102': 104, '103': 105, '104': 106, '105': 107, '106': 108, '107': 109, '108': 110, '109': 111, '110': 112, '111': 113, '112': 114, '113': 115, '114': 116, '115': 117, '116': 118, '117': 119, '118': 120, '119': 121, '120': 122, '121': 123, '122': 124, '123': 125, '124': 126, '125': 127, '126': 128, '127': 129, '128': 130, '129': 131, '130': 132, '131': 133, '132': 134, '133': 135, '134': 136, '135': 137, '136': 138, '137': 139, '138': 140, '139': 141, '140': 142, '141': 143, '142': 144, '143': 145, '144': 146, '145': 147, '146': 148, '147': 149, '148': 150, '149': 151, '150': 152, '151': 153, '152': 154, '153': 155, '154': 156, '155': 157, '156': 158, '157': 159, '158': 160, '159': 161, '160': 162, '161': 163, '162': 164, '163': 165, '164': 166, '165': 167, '166': 168, '167': 169, '168': 170, '169': 171, '170': 172, '171': 173, '172': 174, '173': 175, '174': 176, '175': 177, '176': 178, '177': 179, '178': 180, '179': 181, '180': 182, '181': 183, '182': 184, '183': 185, '184': 186, '185': 187, '186': 188, '187': 189, '188': 190, '189': 191, '190': 192, '191': 193, '192': 194, '193': 195, '194': 196, '195': 197, '196': 198, '197': 199, '198': 200, '199': 201, '200': 202, '201': 203, '202': 204, '203': 205, '204': 206, '205': 207, '206': 208, '207': 209, '208': 210, '209': 211, '210': 212, '211': 213, '212': 214, '213': 215, '214': 216, '215': 217, '216': 218, '217': 219, '218': 220, '219': 221, '220': 222, '221': 223, '222': 224, '223': 225, '224': 226, '225': 227, '226': 228, '227': 229, '228': 230, '229': 231, '230': 232, '231': 233, '232': 234, '233': 235, '234': 236, '235': 237, '236': 238, '237': 239, '238': 240, '239': 241, '240': 242, '241': 243, '242': 244, '243': 245, '244': 246, '245': 247, '246': 248, '247': 249, '248': 250, '249': 251, '250': 252, '251': 253, '252': 254, '253': 255, '254': 256, '255': 257, '256': 258, '257': 259, '258': 260, '259': 261, '260': 262, '261': 263, '262': 264, '263': 265, '264': 266, '265': 267, '266': 268, '267': 269, '268': 270, '269': 271, '270': 272, '271': 273, '272': 274, '273': 275, '274': 276, '275': 277, '276': 278, '277': 279, '278': 280, '279': 281, '280': 282, '281': 283, '282': 284, '283': 285, '284': 286, '285': 287, '286': 288, '287': 289, '288': 290, '289': 291, '290': 292, '291': 293, '292': 294, '293': 295, '294': 296, '295': 297, '296': 298, '297': 299, '298': 300, '299': 301, '300': 302, '301': 303, '302': 304, '303': 305, '304': 306, '305': 307, '306': 308, '307': 309, '308': 310, '309': 311, '310': 312, '311': 313, '312': 314, '313': 315, '314': 316, '315': 317, '316': 318, '317': 319, '318': 320, '319': 321, '320': 322, '321': 323, '322': 324, '323': 325, '324': 326, '325': 327, '326': 328, '327': 329, '328': 330, '329': 331, '330': 332, '331': 333, '332': 334, '333': 335, '334': 336, '335': 337, '336': 338, '337': 339, '338': 340, '339': 341, '340': 342, '341': 343, '342': 344, '343': 345, '344': 346, '345': 347, '346': 348, '347': 349, '348': 350, '349': 351, '350': 352, '351': 353, '352': 354, '353': 355, '354': 356, '355': 357, '356': 358, '357': 359, '358': 360, '359': 361, '360': 362, '361': 363, '362': 364, '363': 365, '364': 366, '365': 367, '366': 368, '367': 369, '368': 370, '369': 371, '370': 372, '371': 373, '372': 374, '373': 375, '374': 376, '375': 377, '376': 378, '377': 379, '378': 380, '379': 381, '380': 382, '381': 383, '382': 384, '383': 385, '384': 386, '385': 387, '386': 388, '387': 389, '388': 390, '389': 391, '390': 392, '391': 393, '392': 394, '393': 395, '394': 396, '395': 397, '396': 398, '397': 399, '398': 400, '399': 401, '400': 402, '401': 403, '402': 404, '403': 405, '404': 406, '405': 407, '406': 408, '407': 409, '408': 410, '409': 411, '410': 412, '411': 413, '412': 414, '413': 415, '414': 416, '415': 417, '416': 418, '417': 419, '418': 420, '419': 421, '420': 422, '421': 423, '422': 424, '423': 425, '424': 426, '425': 427, '426': 428, '427': 429, '428': 430, '429': 431, '430': 432, '431': 433, '432': 434, '433': 435, '434': 436, '435': 437, '436': 438, '437': 439, '438': 440, '439': 441, '440': 442, '441': 443, '442': 444, '443': 445, '444': 446, '445': 447, '446': 448, '447': 449, '448': 450, '449': 451, '450': 452, '451': 453, '452': 454, '453': 455, '454': 456, '455': 457, '456': 458, '457': 459, '458': 460, '459': 461, '460': 462, '461': 463, '462': 464, '463': 465, '464': 466, '465': 467, '466': 468, '467': 469, '468': 470, '469': 471, '470': 472, '471': 473, '472': 474, '473': 475, '474': 476, '475': 477, '476': 478, '477': 479, '478': 480, '479': 481, '480': 482, '481': 483, '482': 484, '483': 485, '484': 486, '485': 487, '486': 488, '487': 489, '488': 490, '489': 491, '490': 492, '491': 493, '492': 494, '493': 495, '494': 496, '495': 497, '496': 498, '497': 499, '498': 500, '499': 501, '500': 502, '501': 503, '502': 504, '503': 505, '504': 506, '505': 507, '506': 508, '507': 509, '508': 510, '509': 511, '510': 512, '511': 513, '512': 514, '513': 515, '514': 516, '515': 517, '516': 518, '517': 519, '518': 520, '519': 521, '520': 522, '521': 523, '522': 524, '523': 525, '524': 526, '525': 527, '526': 528, '527': 529, '528': 530, '529': 531, '530': 532, '531': 533, '532': 534, '533': 535, '534': 536, '535': 537, '536': 538, '537': 539, '538': 540, '539': 541, '540': 542, '541': 543, '542': 544, '543': 545, '544': 546, '545': 547, '546': 548, '547': 549, '548': 550, '549': 551, '550': 552, '551': 553, '552': 554, '553': 555, '554': 556, '555': 557, '556': 558, '557': 559, '558': 560, '559': 561, '560': 562, '561': 563, '562': 564, '563': 565, '564': 566, '565': 567, '566': 568, '567': 569, '568': 570, '569': 571, '570': 572, '571': 573, '572': 574, '573': 575, '574': 576, '575': 577, '576': 578, '577': 579, '578': 580, '579': 581, '580': 582, '581': 583, '582': 584, '583': 585, '584': 586, '585': 587, '586': 588, '587': 589, '588': 590, '589': 591, '590': 592, '591': 593, '592': 594, '593': 595, '594': 596, '595': 597, '596': 598, '597': 599, '598': 600, '599': 601, '600': 602, '601': 603, '602': 604, '603': 605, '604': 606, '605': 607, '606': 608, '607': 609, '608': 610, '609': 611, '610': 612, '611': 613, '612': 614, '613': 615, '614': 616, '615': 617, '616': 618, '617': 619, '618': 620, '619': 621, '620': 622, '621': 623, '622': 624, '623': 625, '624': 626, '625': 627, '626': 628, '627': 629, '628': 630, '629': 631, '630': 632, '631': 633, '632': 634, '633': 635, '634': 636, '635': 637, '636': 638, '637': 639, '638': 640, '639': 641, '640': 642, '641': 643, '642': 644, '643': 645, '644': 646, '645': 647, '646': 648, '647': 649, '648': 650, '649': 651, '650': 652, '651': 653, '652': 654, '653': 655, '654': 656, '655': 657, '656': 658, '657': 659, '658': 660, '659': 661, '660': 662, '661': 663, '662': 664, '663': 665, '664': 666, '665': 667, '666': 668, '667': 669, '668': 670, '669': 671, '670': 672, '671': 673, '672': 674, '673': 675, '674': 676, '675': 677, '676': 678, '677': 679, '678': 680, '679': 681, '680': 682, '681': 683, '682': 684, '683': 685, '684': 686, '685': 687, '686': 688, '687': 689, '688': 690, '689': 691, '690': 692, '691': 693, '692': 694, '693': 695, '694': 696, '695': 697, '696': 698, '697': 699, '698': 700, '699': 701, '700': 702, '701': 703, '702': 704, '703': 705, '704': 706, '705': 707, '706': 708, '707': 709, '708': 710, '709': 711, '710': 712, '711': 713, '712': 714, '713': 715, '714': 716, '715': 717, '716': 718, '717': 719, '718': 720, '719': 721, '720': 722, '721': 723, '722': 724, '723': 725, '724': 726, '725': 727, '726': 728, '727': 729, '728': 730, '729': 731, '730': 732, '731': 733, '732': 734, '733': 735, '734': 736, '735': 737, '736': 738, '737': 739, '738': 740, '739': 741, '740': 742, '741': 743, '742': 744, '743': 745, '744': 746, '745': 747, '746': 748, '747': 749, '748': 750, '749': 751, '750': 752, '751': 753, '752': 754, '753': 755, '754': 756, '755': 757, '756': 758, '757': 759, '758': 760, '759': 761, '760': 762, '761': 763, '762': 764, '763': 765, '764': 766, '765': 767, '766': 768, '767': 769, '768': 770, '769': 771, '770': 772, '771': 773, '772': 774, '773': 775, '774': 776, '775': 777, '776': 778, '777': 779, '778': 780, '779': 781, '780': 782, '781': 783, '782': 784, '783': 785, '784': 786, '785': 787, '786': 788, '787': 789, '788': 790, '789': 791, '790': 792, '791': 793, '792': 794, '793': 795, '794': 796, '795': 797, '796': 798, '797': 799, '798': 800, '799': 801, '800': 802, '801': 803, '802': 804, '803': 805, '804': 806, '805': 807, '806': 808, '807': 809, '808': 810, '809': 811, '810': 812, '811': 813, '812': 814, '813': 815, '814': 816, '815': 817, '816': 818, '817': 819, '818': 820, '819': 821, '820': 822, '821': 823, '822': 824, '823': 825, '824': 826, '825': 827, '826': 828, '827': 829, '828': 830, '829': 831, '830': 832, '831': 833, '832': 834, '833': 835, '834': 836, '835': 837, '836': 838, '837': 839, '838': 840, '839': 841, '840': 842, '841': 843, '842': 844, '843': 845, '844': 846, '845': 847, '846': 848, '847': 849, '848': 850, '849': 851, '850': 852, '851': 853, '852': 854, '853': 855, '854': 856, '855': 857, '856': 858, '857': 859, '858': 860, '859': 861, '860': 862, '861': 863, '862': 864, '863': 865, '864': 866, '865': 867, '866': 868, '867': 869, '868': 870, '869': 871, '870': 872, '871': 873, '872': 874, '873': 875, '874': 876, '875': 877, '876': 878, '877': 879, '878': 880, '879': 881, '880': 882, '881': 883, '882': 884, '883': 885, '884': 886, '885': 887, '886': 888, '887': 889, '888': 890, '889': 891, '890': 892, '891': 893, '892': 894, '893': 895, '894': 896, '895': 897, '896': 898, '897': 899, '898': 900, '899': 901, '900': 902, '901': 903, '902': 904, '903': 905, '904': 906, '905': 907, '906': 908, '907': 909, '908': 910, '909': 911, '910': 912, '911': 913, '912': 914, '913': 915, '914': 916, '915': 917, '916': 918, '917': 919, '918': 920, '919': 921, '920': 922, '921': 923, '922': 924, '923': 925, '924': 926, '925': 927, '926': 928, '927': 929, '928': 930, '929': 931, '930': 932, '931': 933, '932': 934, '933': 935, '934': 936, '935': 937, '936': 938, '937': 939, '938': 940, '939': 941, '940': 942, '941': 943, '942': 944, '943': 945, '944': 946, '945': 947, '946': 948, '947': 949, '948': 950, '949': 951, '950': 952, '951': 953, '952': 954, '953': 955, '954': 956, '955': 957, '956': 958, '957': 959, '958': 960, '959': 961, '960': 962, '961': 963, '962': 964, '963': 965, '964': 966, '965': 967, '966': 968, '967': 969, '968': 970, '969': 971, '970': 972, '971': 973, '972': 974, '973': 975, '974': 976, '975': 977, '976': 978, '977': 979, '978': 980, '979': 981, '980': 982, '981': 983, '982': 984, '983': 985, '984': 986, '985': 987, '986': 988, '987': 989, '988': 990, '989': 991, '990': 992, '991': 993, '992': 994, '993': 995, '994': 996, '995': 997, '996': 998, '997': 999, '998': 1000, '999': 1001, '1000': 1002, '1001': 1003, '1002': 1004, '1003': 1005, '1004': 1006, '1005': 1007, '1006': 1008, '1007': 1009, '1008': 1010, '1009': 1011, '1010': 1012, '1011': 1013, '1012': 1014, '1013': 1015, '1014': 1016, '1015': 1017, '1016': 1018, '1017': 1019, '1018': 1020, '1019': 1021, '1020': 1022, '1021': 1023, '1022': 1024, '1023': 1025, '1024': 1026, '1025': 1027, '1026': 1028, '1027': 1029, '1028': 1030, '1029': 1031, '1030': 1032, '1031': 1033, '1032': 1034, '1033': 1035, '1034': 1036, '1035': 1037, '1036': 1038, '1037': 1039, '1038': 1040, '1039': 1041, '1040': 1042, '1041': 1043, '1042': 1044, '1043': 1045, '1044': 1046, '1045': 1047, '1046': 1048, '1047': 1049, '1048': 1050, '1049': 1051, '1050': 1052, '1051': 1053, '1052': 1054, '1053': 1055, '1054': 1056, '1055': 1057, '1056': 1058, '1057': 1059, '1058': 1060, '1059': 1061, '1060': 1062, '1061': 1063, '1062': 1064, '1063': 1065, '1064': 1066, '1065': 1067, '1066': 1068, '1067': 1069, '1068': 1070, '1069': 1071, '1070': 1072, '1071': 1073, '1072': 1074, '1073': 1075, '1074': 1076, '1075': 1077, '1076': 1078, '1077': 1079, '1078': 1080, '1079': 1081, '1080': 1082, '1081': 1083, '1082': 1084, '1083': 1085, '1084': 1086, '1085': 1087, '1086': 1088, '1087': 1089, '1088': 1090, '1089': 1091, '1090': 1092, '1091': 1093, '1092': 1094, '1093': 1095, '1094': 1096, '1095': 1097, '1096': 1098, '1097': 1099, '1098': 1100, '1099': 1101, '1100': 1102, '1101': 1103, '1102': 1104, '1103': 1105, '1104': 1106, '1105': 1107, '1106': 1108, '1107': 1109, '1108': 1110, '1109': 1111, '1110': 1112, '1111': 1113, '1112': 1114, '1113': 1115, '1114': 1116, '1115': 1117, '1116': 1118, '1117': 1119, '1118': 1120, '1119': 1121, '1120': 1122, '1121': 1123, '1122': 1124, '1123': 1125, '1124': 1126, '1125': 1127, '1126': 1128, '1127': 1129, '1128': 1130, '1129': 1131, '1130': 1132, '1131': 1133, '1132': 1134, '1133': 1135, '1134': 1136, '1135': 1137, '1136': 1138, '1137': 1139, '1138': 1140, '1139': 1141, '1140': 1142, '1141': 1143, '1142': 1144, '1143': 1145, '1144': 1146, '1145': 1147, '1146': 1148, '1147': 1149, '1148': 1150, '1149': 1151, '1150': 1152, '1151': 1153, '1152': 1154, '1153': 1155, '1154': 1156, '1155': 1157, '1156': 1158, '1157': 1159, '1158': 1160, '1159': 1161, '1160': 1162, '1161': 1163, '1162': 1164, '1163': 1165, '1164': 1166, '1165': 1167, '1166': 1168, '1167': 1169, '1168': 1170, '1169': 1171, '1170': 1172, '1171': 1173, '1172': 1174, '1173': 1175, '1174': 1176, '1175': 1177, '1176': 1178, '1177': 1179, '1178': 1180, '1179': 1181, '1180': 1182, '1181': 1183, '1182': 1184, '1183': 1185, '1184': 1186, '1185': 1187, '1186': 1188, '1187': 1189, '1188': 1190, '1189': 1191, '1190': 1192, '1191': 1193, '1192': 1194, '1193': 1195, '1194': 1196, '1195': 1197, '1196': 1198, '1197': 1199, '1198': 1200, '1199': 1201, '1200': 1202, '1201': 1203, '1202': 1204, '1203': 1205, '1204': 1206, '1205': 1207, '1206': 1208, '1207': 1209, '1208': 1210, '1209': 1211, '1210': 1212, '1211': 1213, '1212': 1214, '1213': 1215, '1214': 1216, '1215': 1217, '1216': 1218, '1217': 1219, '1218': 1220, '1219': 1221, '1220': 1222, '1221': 1223, '1222': 1224, '1223': 1225, '1224': 1226, '1225': 1227, '1226': 1228, '1227': 1229, '1228': 1230, '1229': 1231, '1230': 1232, '1231': 1233, '1232': 1234, '1233': 1235, '1234': 1236, '1235': 1237, '1236': 1238, '1237': 1239, '1238': 1240, '1239': 1241, '1240': 1242, '1241': 1243, '1242': 1244, '1243': 1245, '1244': 1246, '1245': 1247, '1246': 1248, '1247': 1249, '1248': 1250, '1249': 1251, '1250': 1252, '1251': 1253, '1252': 1254, '1253': 1255, '1254': 1256, '1255': 1257, '1256': 1258, '1257': 1259, '1258': 1260, '1259': 1261, '1260': 1262, '1261': 1263, '1262': 1264, '1263': 1265, '1264': 1266, '1265': 1267, '1266': 1268, '1267': 1269, '1268': 1270, '1269': 1271, '1270': 1272, '1271': 1273, '1272': 1274, '1273': 1275, '1274': 1276, '1275': 1277, '1276': 1278, '1277': 1279, '1278': 1280, '1279': 1281, '1280': 1282, '1281': 1283, '1282': 1284, '1283': 1285, '1284': 1286, '1285': 1287, '1286': 1288, '1287': 1289, '1288': 1290, '1289': 1291, '1290': 1292, '1291': 1293, '1292': 1294, '1293': 1295, '1294': 1296, '1295': 1297, '1296': 1298, '1297': 1299, '1298': 1300, '1299': 1301, '1300': 1302, '1301': 1303, '1302': 1304, '1303': 1305, '1304': 1306, '1305': 1307, '1306': 1308, '1307': 1309, '1308': 1310, '1309': 1311, '1310': 1312, '1311': 1313, '1312': 1314, '1313': 1315, '1314': 1316, '1315': 1317, '1316': 1318, '1317': 1319, '1318': 1320, '1319': 1321}\n"
     ]
    }
   ],
   "source": [
    "BertVocab = load_obj(file_config['vocab'])\n",
    "med_BertVocab = load_obj(file_config['med_vocab'])\n",
    "ageVocab, _ = age_vocab(max_age=global_params['max_age'], mon=global_params['month'], symbol=global_params['age_symbol'])\n",
    "print(ageVocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      subject_id                                           icd_code  \\\n",
      "0       10000032  [5728, 78959, 07070, V08, SEP, 78959, 07070, 5...   \n",
      "1       10000980  [4111, 78609, 41400, 25001, 4019, SEP, 4280, 4...   \n",
      "2       10001884  [78650, 4019, 49390, SEP, 78650, 4019, SEP, 49...   \n",
      "3       10002013  [78650, 5939, SEP, 41091, SEP, 78650, 25000, V...   \n",
      "4       10002428  [78791, 5589, 4019, 2720, SEP, 51881, 5119, 48...   \n",
      "...          ...                                                ...   \n",
      "6925    19994592  [29570, 2967, SEP, 311, V6284, SEP, R000, SEP,...   \n",
      "6926    19997367  [6930, E9308, V103, SEP, 78609, 2851, 5789, 42...   \n",
      "6927    19997911  [78701, 78791, 42789, 4589, SEP, 78791, SEP, K...   \n",
      "6928    19998330  [51882, 78605, 496, 42731, SEP, 51881, 49121, ...   \n",
      "6929    19998444  [5770, SEP, 78909, 78703, SEP, 78906, SEP, 789...   \n",
      "\n",
      "                                      age_on_admittance  length  \n",
      "0     [52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 5...       5  \n",
      "1     [73, 73, 73, 73, 73, 73, 74, 74, 74, 74, 74, 7...       7  \n",
      "2     [69, 69, 69, 69, 69, 69, 69, 70, 70, 70, 72, 7...      17  \n",
      "3     [56, 56, 56, 57, 57, 57, 57, 57, 57, 57, 59, 5...       8  \n",
      "4     [80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 8...       6  \n",
      "...                                                 ...     ...  \n",
      "6925  [41, 41, 41, 41, 41, 41, 44, 44, 44, 44, 45, 4...       7  \n",
      "6926  [63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 6...      10  \n",
      "6927  [83, 83, 83, 83, 83, 85, 85, 86, 86, 86, 86, 8...       5  \n",
      "6928  [72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 7...       5  \n",
      "6929       [38, 38, 38, 38, 38, 38, 38, 38, 38, 39, 39]       5  \n",
      "\n",
      "[6930 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_parquet(file_config['data'])\n",
    "# remove patients with visits less than min visit\n",
    "# print(data)\n",
    "data['length'] = data['icd_code'].apply(lambda x: len([i for i in range(len(x)) if x[i] == 'SEP']))\n",
    "data = data[data['length'] >= global_params['min_visit']]\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data)\n",
    "Dset = MLMLoader(data, BertVocab['token2idx'], age2idx = ageVocab, med_token2idx=med_BertVocab['token2idx'], max_len=train_params['max_len_seq'], code='icd_code', age='age_on_admittance', med='med')\n",
    "\n",
    "trainload = DataLoader(dataset=Dset, batch_size=train_params['batch_size'], shuffle=True, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79917\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "model_config = {\n",
    "    'vocab_size': len(BertVocab['token2idx'].keys()), # number of disease + symbols for word embedding\n",
    "    'med_vocab_size': len(med_BertVocab['token2idx'].keys()), # OWN EMBEDDINGS\n",
    "\n",
    "    'hidden_size': 288, # word embedding and seg embedding hidden size\n",
    "    'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "    'age_vocab_size': len(ageVocab.keys()), # number of vocab for age embedding\n",
    "    'max_position_embedding': train_params['max_len_seq'], # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.1, # dropout rate\n",
    "    'num_hidden_layers': 6, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 12, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.1, # multi-head attention dropout rate\n",
    "    'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range\n",
    "}\n",
    "\n",
    "print(model_config['vocab_size'])\n",
    "print(model_config['max_position_embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = BertConfig(model_config)\n",
    "model = BertForMaskedLM(conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "model = model.to(train_params['device'])\n",
    "optim = adam(params=list(model.named_parameters()), config=optim_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(label, pred):\n",
    "    logs = nn.LogSoftmax()\n",
    "    label=label.cpu().numpy()\n",
    "    ind = np.where(label!=-1)[0]\n",
    "    truepred = pred.detach().cpu().numpy()\n",
    "    truepred = truepred[ind]\n",
    "    truelabel = label[ind]\n",
    "    truepred = logs(torch.tensor(truepred))\n",
    "    outs = [np.argmax(pred_x) for pred_x in truepred.numpy()]\n",
    "    precision = skm.precision_score(truelabel, outs, average='micro')\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(e, loader):\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    cnt= 0\n",
    "    start = time.time()\n",
    "\n",
    "    for step, batch in enumerate(loader):\n",
    "        cnt +=1\n",
    "        batch = tuple(t.to(train_params['device']) for t in batch)\n",
    "        age_ids, input_ids, posi_ids, segment_ids, attMask, masked_label = batch\n",
    "        print(age_ids)\n",
    "        print(input_ids)\n",
    "        loss, pred, label = model(input_ids, age_ids, segment_ids, posi_ids,attention_mask=attMask, masked_lm_labels=masked_label)\n",
    "        if global_params['gradient_accumulation_steps'] >1:\n",
    "            loss = loss/global_params['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "        \n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        \n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        if step % 200==0:\n",
    "            print(\"epoch: {}\\t| cnt: {}\\t|Loss: {}\\t| precision: {:.4f}\\t| time: {:.2f}\".format(e, cnt, temp_loss/2000, cal_acc(label, pred), time.time()-start))\n",
    "            temp_loss = 0\n",
    "            start = time.time()\n",
    "            \n",
    "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            \n",
    "\n",
    "    print(\"** ** * Saving fine - tuned model ** ** * \")\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "    create_folder(file_config['model_path'])\n",
    "    output_model_file = os.path.join(file_config['model_path'], file_config['model_name'])\n",
    "\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)\n",
    "        \n",
    "    cost = time.time() - start\n",
    "    return tr_loss, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[80, 80, 80,  ...,  0,  0,  0],\n",
      "        [77, 77, 77,  ...,  0,  0,  0],\n",
      "        [74, 74, 74,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [49, 49, 49,  ...,  0,  0,  0],\n",
      "        [90, 90, 90,  ...,  0,  0,  0],\n",
      "        [51, 51, 51,  ...,  0,  0,  0]])\n",
      "tensor([[    1,     3,     3,  ...,     4,     4,     4],\n",
      "        [    1,     3,     3,  ...,     4,     4,     4],\n",
      "        [    2,     3,     3,  ...,     4,     4,     4],\n",
      "        ...,\n",
      "        [    1,     3,  7036,  ...,     4,     4,     4],\n",
      "        [    1, 15601,     0,  ...,     4,     4,     4],\n",
      "        [    1,     3,     0,  ...,     4,     4,     4]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m f\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m50\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     loss, time_cost \u001b[39m=\u001b[39m train(e, trainload)\n\u001b[1;32m      5\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m/\u001b[39mdata_len\n\u001b[1;32m      6\u001b[0m     f\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(e, loss, time_cost))\n",
      "Cell \u001b[0;32mIn[13], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(e, loader)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(age_ids)\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(input_ids)\n\u001b[0;32m---> 14\u001b[0m loss, pred, label \u001b[39m=\u001b[39m model(input_ids, age_ids, segment_ids, posi_ids,attention_mask\u001b[39m=\u001b[39;49mattMask, masked_lm_labels\u001b[39m=\u001b[39;49mmasked_label)\n\u001b[1;32m     15\u001b[0m \u001b[39mif\u001b[39;00m global_params[\u001b[39m'\u001b[39m\u001b[39mgradient_accumulation_steps\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m     16\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m/\u001b[39mglobal_params[\u001b[39m'\u001b[39m\u001b[39mgradient_accumulation_steps\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/research/BEHRT/task/../model/MLM.py:118\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, age_ids, seg_ids, posi_ids, attention_mask, masked_lm_labels)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, age_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, seg_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, posi_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, attention_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, masked_lm_labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 118\u001b[0m     sequence_output, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(input_ids, age_ids, seg_ids, posi_ids, attention_mask,\n\u001b[1;32m    119\u001b[0m                                    output_all_encoded_layers\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    120\u001b[0m     prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls(sequence_output)\n\u001b[1;32m    122\u001b[0m     \u001b[39mif\u001b[39;00m masked_lm_labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/research/BEHRT/task/../model/MLM.py:99\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, age_ids, seg_ids, posi_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m     96\u001b[0m extended_attention_mask \u001b[39m=\u001b[39m extended_attention_mask\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39m\u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters())\u001b[39m.\u001b[39mdtype)  \u001b[39m# fp16 compatibility\u001b[39;00m\n\u001b[1;32m     97\u001b[0m extended_attention_mask \u001b[39m=\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m extended_attention_mask) \u001b[39m*\u001b[39m \u001b[39m-\u001b[39m\u001b[39m10000.0\u001b[39m\n\u001b[0;32m---> 99\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(input_ids, age_ids, seg_ids, posi_ids)\n\u001b[1;32m    100\u001b[0m encoded_layers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(embedding_output,\n\u001b[1;32m    101\u001b[0m                               extended_attention_mask,\n\u001b[1;32m    102\u001b[0m                               output_all_encoded_layers\u001b[39m=\u001b[39moutput_all_encoded_layers)\n\u001b[1;32m    103\u001b[0m sequence_output \u001b[39m=\u001b[39m encoded_layers[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/Envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/research/BEHRT/task/../model/MLM.py:29\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, word_ids, age_ids, seg_ids, posi_ids, age)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mif\u001b[39;00m posi_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     posi_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(word_ids)\n\u001b[0;32m---> 29\u001b[0m word_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embeddings(word_ids)\n\u001b[1;32m     30\u001b[0m segment_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msegment_embeddings(seg_ids)\n\u001b[1;32m     31\u001b[0m age_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mage_embeddings(age_ids)\n",
      "File \u001b[0;32m~/Envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Envs/torch/lib/python3.9/site-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/Envs/torch/lib/python3.9/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "f = open(os.path.join(file_config['model_path'], file_config['file_name']), \"w\")\n",
    "f.write('{}\\t{}\\t{}\\n'.format('epoch', 'loss', 'time'))\n",
    "for e in range(50):\n",
    "    loss, time_cost = train(e, trainload)\n",
    "    loss = loss/data_len\n",
    "    f.write('{}\\t{}\\t{}\\n'.format(e, loss, time_cost))\n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "79bd467a5a314cc884c073891ebda5c60460f565ef31ac54686cd06fc93b69d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
